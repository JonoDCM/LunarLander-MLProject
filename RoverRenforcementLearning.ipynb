{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw_irVPS_POX"
      },
      "outputs": [],
      "source": [
        "pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of7PWFEmBJfU"
      },
      "outputs": [],
      "source": [
        "pip install swig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVt9RkBTBNTb"
      },
      "outputs": [],
      "source": [
        "pip install \"gymnasium[box2d]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETVoSX306eg1"
      },
      "source": [
        "All of the imports used in this project:\n",
        "\n",
        "*   gymnasium: used to get an environment for the project.\n",
        "*   numpy: used for numerical operations.\n",
        "*   tourch: PyTorhc is used for building and also training the nueral networks.\n",
        "*   collections: used for the replay buffer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re6lB9CrBC9p"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRShDp6FBiy1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from collections import deque, namedtuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TzU9Y9l5ppX"
      },
      "source": [
        "Lunar Lander Environment, this setup os the default one given in the Gymnasium Documentaion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2t28kDACOKw"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGRJyu2b8PFH"
      },
      "source": [
        "Gets the shape size of the state space, and also the number of actions\n",
        "\n",
        "Action Space (discrete):\n",
        "- 0: Do nothing\n",
        "- 1: Fire left orientation engine\n",
        "- 2: Fire main engine\n",
        "- 3: Fire right orientation engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd_5z_4kNicd",
        "outputId": "01f415f7-3a59-4335-8c4e-2006da4ba333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State shape:  (8,)\n",
            "State size:  8\n",
            "Number of actions:  4\n"
          ]
        }
      ],
      "source": [
        "state_shape = env.observation_space.shape\n",
        "state_size = env.observation_space.shape[0]\n",
        "number_actions = env.action_space.n\n",
        "print('State shape: ', state_shape)\n",
        "print('State size: ', state_size)\n",
        "print('Number of actions: ', number_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E91T9-hy72hh"
      },
      "source": [
        "Hyperparameters, these define:\n",
        "\n",
        "*   Learing rate for the optimizer (Î±)\n",
        "*   Mini-batch size for sampling experiences\n",
        "*   Discount factiors for comming rewards.(Î³)\n",
        "*   Replay Buffer capazity\n",
        "*   Interpolation factos for soft updates of the target network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZYdDykpOAhw"
      },
      "outputs": [],
      "source": [
        "learning_rate = 5e-4\n",
        "minibatch_size = 100\n",
        "discount_factor = 0.99\n",
        "replay_buffer_size = int(1e5)\n",
        "interpolation_parameter = 1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uenx7Ahv9CIp"
      },
      "source": [
        "The class ANN defienes a 3-layer neural network:\n",
        "\n",
        "*   Input layer: state_size\n",
        "*   Hidden layers: 64 units each\n",
        "*   Output layer: action_size (Q-values for each of the actions)\n",
        "\n",
        "This is the Q-network, estimating:\n",
        "\n",
        "ð‘„\n",
        "(\n",
        "ð‘ \n",
        ",\n",
        "ð‘Ž\n",
        ")\n",
        "â‰ˆ\n",
        "expectedÂ returnÂ startingÂ fromÂ state\n",
        "ð‘ \n",
        "Â andÂ takingÂ action\n",
        "ð‘Ž\n",
        "\n",
        "So it outputs one Q-value per possible action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7_HD5OMPsG6"
      },
      "outputs": [],
      "source": [
        "class ANN(nn.Module):\n",
        "  def __init__(self, state_size, action_size, seed = 42):\n",
        "    super(ANN, self).__init__()\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    self.fc1 = nn.Linear(state_size, 64)\n",
        "    self.fc2 = nn.Linear(64,64)\n",
        "    self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.fc1(state)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    return self.fc3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNKwrY0d9y_p"
      },
      "source": [
        "This class is the Replay Buffer that:\n",
        "\n",
        "\n",
        "*   Stores tuples of experiences: (state, action. reward, next_state, done)\n",
        "*   Allows random sampling for experience replay\n",
        "\n",
        "Sampling returns: states, actions, rewards, next_states, dones\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5G_damWQCAW"
      },
      "outputs": [],
      "source": [
        "class Replaybuffer(object):\n",
        "  def __init__(self, capacity):\n",
        "    self.capacity = capacity\n",
        "    self.memory = []\n",
        "\n",
        "  def push(self, event):\n",
        "    self.memory.append(event)\n",
        "    if len(self.memory) > self.capacity:\n",
        "      del self.memory[0]\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    experiences = random.sample(self.memory, batch_size)\n",
        "\n",
        "    states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float()\n",
        "    actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long()\n",
        "    rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float()\n",
        "    next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float()\n",
        "    dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float()\n",
        "\n",
        "    return states, next_states, actions, rewards, dones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lzJBWwrzVoF"
      },
      "source": [
        "The class Agent (DQN Agent) encapsulates the reinforment learning logic:\n",
        "\n",
        "*   Initializes two networks: Local and Target\n",
        "\n",
        "local_qnetwork: Used for choosing actions\n",
        "\n",
        "target_qnetwork: Used for stable target calculation\n",
        "*   Uses epsilon-greedy for exploration\n",
        "*   Stores experiences and samples mini-batches for training\n",
        "*   Performs soft updates in the target network\n",
        "\n",
        "Step Function:\n",
        "Stores the transition and learns every 4 steps if enough memory exists.\n",
        "\n",
        "Act Function: Selects an Action to be taken\n",
        "- Convert NumPy state to Torch tensor and move it to GPU/CPU.\n",
        "- unsqueeze(0) adds a batch dimension, making shape (1, state_size).\n",
        "\n",
        "Learning / Update Rule\n",
        "Key equation:\n",
        "\n",
        "ð‘„\n",
        "^\n",
        "(\n",
        "ð‘ \n",
        ",\n",
        "ð‘Ž\n",
        ")\n",
        "=\n",
        "ð‘Ÿ\n",
        "+\n",
        "ð›¾\n",
        "â‹…\n",
        "max\n",
        "â¡\n",
        "ð‘Ž\n",
        "â€²\n",
        "ð‘„\n",
        "target\n",
        "(\n",
        "ð‘ \n",
        "â€²\n",
        ",\n",
        "ð‘Ž\n",
        "â€²\n",
        ")\n",
        "(TDÂ Target)\n",
        "Q\n",
        "^\n",
        "â€‹\n",
        " (s,a)=r+Î³â‹…\n",
        "a\n",
        "â€²\n",
        "\n",
        "max\n",
        "â€‹\n",
        " Q\n",
        "target\n",
        "â€‹\n",
        " (s\n",
        "â€²\n",
        " ,a\n",
        "â€²\n",
        " )(TDÂ Target)\n",
        "\n",
        " Soft Update:\n",
        "\n",
        "ðœƒ\n",
        "target\n",
        "â†\n",
        "ðœ\n",
        "â‹…\n",
        "ðœƒ\n",
        "local\n",
        "+\n",
        "(\n",
        "1\n",
        "âˆ’\n",
        "ðœ\n",
        ")\n",
        "â‹…\n",
        "ðœƒ\n",
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWZKnXWss9OD"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.local_qnetwork = ANN(state_size, action_size).to(self.device)\n",
        "    self.target_qnetwork = ANN(state_size, action_size).to(self.device)\n",
        "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n",
        "    self.memory = Replaybuffer(replay_buffer_size)\n",
        "    self.t_step = 0\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    self.memory.push((state, action, reward, next_state, done))\n",
        "    self.t_step = (self.t_step + 1) % 4\n",
        "    if self.t_step == 0:\n",
        "      if len(self.memory.memory) > minibatch_size:\n",
        "        experiences = self.memory.sample(100)\n",
        "        self.learn(experiences, discount_factor)\n",
        "\n",
        "  def act(self, state, epsilon = 0.):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "    self.local_qnetwork.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.local_qnetwork(state)\n",
        "    self.local_qnetwork.train()\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "  def learn(self, experiences, discount_factor):\n",
        "    states, next_states, actions, rewards, dones = experiences\n",
        "\n",
        "    states = states.to(self.device)\n",
        "    next_states = next_states.to(self.device)\n",
        "    actions = actions.to(self.device)\n",
        "    rewards = rewards.to(self.device)\n",
        "    dones = dones.to(self.device)\n",
        "\n",
        "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n",
        "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "    loss = F.mse_loss(q_expected, q_targets)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n",
        "\n",
        "\n",
        "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FJo99rwt3C2"
      },
      "outputs": [],
      "source": [
        "agent = Agent(state_size, number_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v_M3-kxp-yS"
      },
      "source": [
        "Reset the environment\n",
        "\n",
        "For each time step:\n",
        "\n",
        "- Choose action with epsilon-greedy\n",
        "\n",
        "- Perform the action â†’ observe reward and next state\n",
        "\n",
        "- Store experience\n",
        "\n",
        "- Learn if ready\n",
        "\n",
        "Decay epsilon:\n",
        "\n",
        "Ïµ=max(Ïµ\n",
        "min\n",
        "â€‹\n",
        " ,Ïµâ‹…Ïµ\n",
        "decay\n",
        "â€‹\n",
        " )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiDizZ_E2K23",
        "outputId": "b4e35b82-8990-401a-a7fc-53fa29816485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: -101.57\n",
            "Episode 200\tAverage Score: -36.69\n",
            "Episode 300\tAverage Score: 29.90\n",
            "Episode 400\tAverage Score: 121.26\n",
            "Episode 442\tAverage Score: 200.11\n",
            "Environment had solved been solved. \tAverage Score: 342.00\n"
          ]
        }
      ],
      "source": [
        "number_episodes = 2000\n",
        "maximum_number_timesteps_per_episode = 1000\n",
        "epsilon_starting_value  = 1.0\n",
        "epsilon_ending_value  = 0.01\n",
        "epsilon_decay_value  = 0.995\n",
        "epsilon = epsilon_starting_value\n",
        "scores_on_100_episodes = deque(maxlen = 100)\n",
        "\n",
        "for episode in range(1, number_episodes + 1):\n",
        "  state, _ = env.reset()\n",
        "  score = 0\n",
        "  for t in range(maximum_number_timesteps_per_episode):\n",
        "    action = agent.act(state, epsilon)\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    agent.step(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if done:\n",
        "      break\n",
        "  scores_on_100_episodes.append(score)\n",
        "  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
        "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n",
        "  if episode % 100 == 0:\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
        "  if np.mean(scores_on_100_episodes) >= 200.0:\n",
        "    print('\\nEnvironment had solved been solved. \\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
        "    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkuXiUdDqWt0"
      },
      "source": [
        "Shows video of the trainmed agent in the environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0hgUaUFw2Q98"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'LunarLander-v3')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data=f'''\n",
        "        <video width=\"640\" height=\"480\" controls>\n",
        "          <source src=\"data:video/mp4;base64,{encoded.decode('ascii')}\" type=\"video/mp4\">\n",
        "        </video>\n",
        "'''))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}